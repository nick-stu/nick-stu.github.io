---
title: '数据挖掘导论笔记(6): 聚类分析-基本概念和算法'
date: 2018-08-15 19:01:08
tags:
- 数据挖掘
- 机器学习
- 读书笔记
---

# 概述

## 什么是聚类分析

通俗来讲是将数据对象分组（划分成簇），目标为组内对象相似，不同组对象不同。聚类的过程可以说是“主观的”，需要依赖于数据特性和期望结果

## 不同的聚类类型

### 层次的（嵌套的）与划分的（非嵌套的）

划分聚类表示简单讲总的数据对象集合划分为不重叠的子集（想象为切饼前与切饼后）
层次聚类是嵌套的，类似于一棵树，一个结点是其子女的合并。（切饼的几个过程）
层级聚类可以看成划分聚类的序列，划分聚类可以通过取序列的任意成员得到。

> 互斥的、重叠的与模糊的：前两个概念容易理解，关于模糊聚类是指，每个对象以一权值隶属于每个簇（通常权值之和等于1，如0.5的权值属于簇1,0.3权值属于簇2,0.2权值属于簇3），可以转化为互斥聚类

> 完全的与部分的：部分意味着不是所有对象都一定要有归属簇

## 不同的簇类型

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\1.png %}

 - 明显分离的：每个对象到同簇中的任意一个对象都比到异簇的任意对象近，是种理想的模型（到任一比到任一近）
 - 基于原型的：原型可以理解为中心，每个对象到其簇中的中心都比异簇的中心近，例子有kmeans（到中心比到中心近）
 - 基于图的：可以联想图论中的连通分量之类的概念，典型例子有基于邻近的簇，这里定义“相连”是通过两点在某一阈值内，簇中每个对象到同簇中某个对象的距离大于到异簇中任一对象的距离。适合不规则形状的簇，不过会出现上图c中两个簇由于中间的“桥”被合并（到某一比到任一近）
 - 基于密度：使用与不规则形状簇，典型例子有DBSCAN
 - 共同性质的（概念簇）：这个范围较广（包括上述的簇定义），关键在于簇的性质，这里的性质概念抽象

# K均值

## 基本K均值算法

kmeans是基于原型的、划分的聚类算法（技术实现这里不详述，这里分析一下部分步骤）

### 指派点到最近的质心

将点指派到最近的质心，需要量化“最近”这个概念。对于欧式空间中的点常用欧氏距离，对文档使用余弦相似性

### 质心与目标函数

重新计算每个簇的质心这一步可不是简简单单求中心点就行，质心是基于邻近性度量和聚类目标函数得出的。我们常见的欧几里得空间目标函数为误差平方和SSE（Sum of the Squared Error）$=\sum^K_{i=1}\sum_{x\in C_i}dist(c_i,x)^2$，邻近性度量为欧式距离，可以证明正是求中心点（均值）。对于文档数据来说，目标函数为凝聚度，邻近性度量为余弦相似性，其质心也是均值。

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\2.png %}

#### 选择初始质心

常采取随机选取初始质心的方式（多次运行，选取具有最小SSE的簇集）

> 注意kmeans上述的两个步骤是不能找到全局最优的，糟糕的初始质心会影响聚类效果（如下图）

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\4.png %}
上图这种情况下，最终聚类结果会将右边的两类聚为一簇。随着簇个数的增加，多次随机运行也很难避免这种情况。

下面有两种解决方法：

 - 先取一部分样本，进行层次聚类提取出k个类的质心，作为kmeans的初始质心
 - 随机选取第一个，之后都选取和前一个最远的点（这样就能避免初始质心尽量分散），不过可能会选中离群点，所以先使用点样本进行操作（即从总的样本中抽样），这样点样本中会选出稠密区域的点，同时也减少选中离散点的可能，计算量减小

## K均值：附加的问题

### 处理空簇

意思是存在某个质心没有分派给它的样本点，这种情况会选择一个替补质心，一种方法是选距离当前所有质心最远的点（消除当前对总SSE影响最大的点）；另一种方法是从具有最大SSE的簇中选择一个作为替补质心（将导致簇分裂降低总SSE）

### 离群点

离群点会影响迭代后质心的位置，导致SSE的偏高，情况允许的条件下，可以提前发现离群点并删去；也可以在后处理中识别离群点（如记录每个点对SSE的影响），此外有时会删除那些很小的簇（常常意味着离群点的簇）

### 用后处理降低SSE

有两种方式去降低SSE

增加簇个数：
 - 分裂具备最大簇SSE的簇
 - 引进新质心
 
减少簇个数：
 - 拆散簇，将其点重新分配
 - 合并簇
 

## 二分K均值

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\3.png %}
每次迭代选择一个簇作为局部，对局部进行kmeans（k=2），上图中橙色区域代表我们选中的“局部”，可以看出这种方法受质心初始化的影响不大

## 优缺点

不能处理非球形簇、不同尺寸和不同密度的簇；对于离群点较敏感；且仅限于具有中心概念的数据。
{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\5.png %}

# 凝聚层次聚类

## 基本凝聚层次聚类算法

```
初始时，每个点都是一个簇
如果需要，计算邻近度矩阵
repeat
    合并最接近的两个簇
    更新邻近性矩阵，以反映新的簇与原来的簇之间的邻近性
until 仅剩下一个簇
```

### 定义簇之间的邻近性

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\6.png %}
除上面几种之外，还有基于质心的、Ward方法（度量基于合并后的SSE增量）等方法

单链适用于**非椭圆形状簇**，对噪声/离群点敏感；全链对噪声/离群点不敏感，不过可能会造成大簇破裂

以上的方法都是Lance-Williams公式的一种选择

## 层次聚类的主要问题

层次聚类不能视为全局优化目标函数，同时也没有局部极小问题或初始点选择问题，不过复杂度较大

此外，使用ward方法、组平均、质心等方法时，如何对待不同大小的簇也是一个问题（有两种方式：加权方法平等对待所有簇，非加权方法考虑簇的点数）

层次聚类只是追求了局部最优，一旦合并就不能撤销

# DBSCAN

Density-based spatial clustering of applications with noise，是典型的基于密度聚类

## 传统的密度：基于中心的方法

在基于中心的方法中，密度定义为点的Eps半径之内点的个数。基于定义，有核心点（邻域内点个数超过阈值MinPts）、边界点（不是核心点，但落在核心点邻域）和噪声点（非上面两种的点）

## DBSCAN算法

大致过程为删去噪声点，将互被包含在邻域中的核心点聚为一簇，将边界点指派到与之关联的核心点的簇中

### 参数选择

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\7.png %}
基本方法是画出上图，选定急剧变化的位置为Eps，此外一般对于二维数据，选定的k就是4


### 优缺点

能处理任意形状大小的簇，但簇密度差异太大会带来麻烦，此外对于高纬数据的密度难以定义（在下一篇中会讲如何处理）

# 簇评估

也叫簇确认（cluster validation）
存在各种各样的簇类型，需要不同的评估度量，这里只介绍一些常用的

## 概述

簇确认的重要问题：
 1. 确定数据集的聚类趋势，即识别数据中是否实际存在非随机结构
 2. 确定簇的正确个数
 3. 不引用附加信息，评估聚类分析结果对数据拟合的情况
 4. 将聚类结果与已知客观结果（类标签）比较
 5. 比较两个簇集

## 非监督簇评估：使用凝聚度和分离度

K个簇集合的总体簇有效性表示为个体簇有效性的加权和$$overall\ validity=\sum^K_{i=1}w_ivalidity(C_i)$$其中的validity函数可以是凝聚度（cohesion）、分离度（separation）等
{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\8.png %}

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\9.png %}
<center>凝聚度和分离度的基于图的观点</center>

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\10.png %}
<center>凝聚度和分离度的基于原型的观点</center>
基于原型的分离度量有两种做法，下图是当邻近度用欧氏距离度量的情况（第二式子假设簇大小相等）
{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\12.png %}

基于原型的和基于图的凝聚度存在如下联系：
{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\11.png %}

## 非监督簇评估：使用邻近度矩阵

考察相似度矩阵和基于簇标号的相似度矩阵（理想版本，即簇内点相似度为1，而异簇相似度为0，具有块对角结构）的相关性来评估聚类效果，高度相关则证明簇内点互相接近，而低相关性表明相反情况。

也可以将相似度矩阵可视化

## 轮廓系数（silhouette coefficient）

结合了凝聚度和分离度$$对于第i个数据对象轮廓系数s_i=(b_i-a_i)/max(a_i,b_i)$$
$a_i$为它到簇中其他对象的平均距离，$b_i$为min{到异簇所有对象的平均距离}，轮廓系数值域为-1到1，出现负值代表到簇内比到簇外还远

## 确定正确的簇个数

{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\13.png %}
利用某种聚类方法画出上图

## 聚类趋势

即分析数据集中是否存在簇，还是随机分布。
一般会对其尝试各种聚类方法，如果效果都不佳则证明确实没有簇，但更方便的是使用统计检验来检验空间随机性，如Hopkins统计量$$H=\frac{\sum^p_{i=1}w_i}{\sum^p_{i=1}u_i+\sum^p_{i=1}w_i}$$其中，我们的做法是从数据集抽取p个样本点，此外再产生随机分布在数据空间上的p个点，上式中的u是人工产生点离原数据集的最近邻距离，w是样本点离原数据集的最近邻距离。

若接近随机分布，则H大约为0.5，反之接近0或1

## 簇有效性的监督度量

### 簇有效性的面向分类的度量

熵、纯度、精度、召回率和F度量等都可以作为评估度量

#### 熵

每个簇由**单个类**（泛指）的对象组成的程度。计算簇i的成员属于类j的概率$p_{ij}=m_{ij}/m_i$，其中$m_{ij}$表示簇i中类j的对象个数，用标准公式$e_i=-\sum_{j=1}^Lp_{ij}log_2p_{ij}$，最后簇集合总熵用每个簇的熵加权和得出（权值为簇大小/总大小）

#### 纯度

簇包含**单个类**（泛指）的对象。$p_i=\max_jp_{ij}$，总纯度算法和熵的相同

#### 精度

簇中一个**特定类**（特指）的对象所占比例

#### 召回率

簇包含一个特定类的所有对象的程度（有点类似于精度的倒数），簇i关于类j的召回率为$recall(i,j)=m_{ij}/m_j$

#### F度量

精度和召回率的组合，度量簇在多大程度上只包含一个特定类的对象和包含类的所有对象。簇i关于类j的F度量$$F(i,j)=(2\times precision(i,j)\times recall(i,j))/(precision(i,j)+recall(i,j))$$

### 簇有效性的面向相似性的度量

两个概念：理想的簇相似度矩阵和理想的类相似度矩阵，通过计算两个矩阵的相关度作为簇有效性的度量，称为$\Gamma$统计量
{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\14.png %}

扩展开来，可以使用其他二元相似性度量，如Rand统计量和Jaccard系数
{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\15.png %}

### 层次聚类的簇有效性

前面讲的有效性监督度量都针对划分聚类，这里介绍关于层次聚类的。
$$F=\sum_j\frac{m_j}{m}\max_iF(imj)$$
其中，是对每个类j计算簇层次每个簇的F度量，取最大的F度量，最后加权平均每类的F度量（注意是针对所有层的所有簇）

## 评估簇有效性度量的显著性

我们的评估方法最后都无非得出一个数值，如何确定这个数值表达了什么？（显著性）特别是当数值没有最大值或最小值作为参考时。

一般常用的方法是用统计学解释，试图确定随机情况得到对应值的可能性有多大
{% qnimg 数据挖掘导论笔记-6-聚类分析-基本概念和算法\16.png %}
如上图是对随机数据集做500次聚类之后的结果（每次数据集都不同）