---
title: '数据挖掘导论笔记(4): 关联分析-基本概念和算法'
date: 2018-08-13 11:34:06
tags:
- 数据挖掘
- 机器学习
- 读书笔记
---


{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\23.png %}
（本文只算介绍了上图的少部分）

> 关联分析（association analysis），用于发现隐藏在数据中有意义的联系。联系可以用关联规则和频繁项集表示

应用广泛，如市场促销、库存管理、顾客关系管理、生物信息学、医疗诊断、网页挖掘和科学数据分析等等

# 问题定义

## 支持度和置信度

以购物篮数据为例，其中每个事务购买物品的集合称为项集。对于X -> Y的关联规则，其中X和Y是不相交的项集，关联规则的强度可以由支持度和置信度度量。
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\1.png %}

低支持度的规则意味着可能只是偶尔出现；置信度度量规则的可靠性

## 规则的发现

遍历的计算量非常大，一般我们遵循下面的流程：

 1. 频繁项集产生：发现满足最小支持度阈值的所有项集，称为频繁项集（frequent itemset）
 2. 规则的产生：从频繁项集中提取所有高置信度的规则，称为强规则（strong rule）

# 频繁项集的产生

如果逐个计算所有可能的项集，对于一个包含k个项的数据集的候选项集数为$M=2^k-1$，此外还要同N个事务数中w个项逐个比较，复杂度大

有两种方法：

 1. 减少候选项集的数目：先验原理
 2. 减少比较次数

## 先验原理

> 如果一个项集是频繁的，则它的子集一定是频繁的；如果一个项集是非频繁的，则它的超集一定非频繁

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\2.png %}

上图右侧这种称为基于支持度的剪枝，它依赖于支持度度量的反单调性，这里的反单调性即对于所有的X包含Y，但Y支持度大于X（反之即为单调性）。
上述方法有效对候选项集的指数搜索空间进行了剪枝

## Apriori算法的频繁项集产生

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\3.png %}

apriori-gen函数负责产生新的候选项集，下节介绍；subset确定扫描事务中的候选项集进行计数，下下节介绍

## 候选的产生与剪枝

apriori-gen函数包含两个步骤：

 1. 候选项集的产生（基于上一轮的频繁项集）
 2. 候选项集的剪枝（注意这里和后面subset进行的计数之后剪枝并不冲突，这里的剪枝意思是基于我们前面的先验原理进行的剪枝，即子集若非频繁，则含有该子集的候选项集会被剪枝，可以想象成粗剪枝）

针对第一步这里介绍$F_{k-1}\times F_{k-1}$方法，我们将每个项集里面的项都按字典序排列，并且要求对于一对频繁(k-1)-项集，仅当它们的前k-2项都相同，且最后一项不同，才能把它们合并为新的候选k-项集。字典序避免生成重复候选

第二步减少之后计算支持度的工作量，注意上面那步只确保了它的两个(k-1)子集是频繁的，其余的(k-1)个子集还需要我们进行检查剪枝

## 支持度计数

采用的方法是枚举每个事务包含的项集：

首先介绍枚举方式：将事务中的项按字典序排列，先指定最小项，后跟随较大的项，从左到右指定，逐渐确定前缀，逐层分裂
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\4.png %}

接下来介绍如何利用Hash数进行支持度计数，主要流程为[利用候选项集建hash树](https://blog.csdn.net/owengbs/article/details/7626009)，之后让每个事务按照上面的分裂方式跟着树的分支分解，找到最后的叶结点。
首先，将先前得到的候选项集散列为hash树（需要指定hash函数，这里为h(p)=p mod 3，同时也指定叶结点最多含有多少个候选项集，是为了减小最后在叶结点进行比较的计算量）

之后的[事务分解过程](https://www.zhihu.com/question/27969345/answer/69149156)如下（注意分解也是按照hash散列，这样才能找到对应的叶结点）：
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\5.png %}

# 规则产生

将频繁项集分裂就能构成关联规则，每一个频繁项集能够分裂为许多规则，这里也要进行剪枝（基于置信度）

> 如果规则X -> Y-X不满足置信度阈值，则X' -> Y-X'的规则也一定不满足，其中X'是X的子集

上面的定理通过置信度的求解方法很容易证明（这里求置信度不需要再去遍历事务集，因为可以利用之前求的支持度求置信度）

## Apriori算法中规则的产生

逐层产生规则，每层对应规则后件的项数，随着层数增加，后件项数变大，利用上面的定理，可以方便的进行剪枝
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\6.png %}
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\7.png %}
ps:这里的apriori-gen函数内不需要剪枝

# 频繁项集的紧凑表示

## 极大频繁项集

> 极大频繁项集（maximal frequent itemset）的直接超集都不是频繁的。

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\8.png %}
意味着**一个极大频繁项集的集合**是一个**能够表示所有频繁项集（通过子集推出）的**最小集合
缺点是不能提供子集的支持度

## 闭频繁项集

通过闭频繁项集能够具备表示支持度的功能

> 闭项集（closed itemset）的直接超集都不具有和它相同的支持度计数

根据闭项集很容易知道闭频繁项集的定义，事实上极大频繁项集是特殊的闭频繁项集
可以利用闭频繁项集的支持度推出非闭的频繁项集支持度（因为非闭意味着它与其某个直接超集的支持度相同，根据反单调性，求其直接超集支持度的最大值即可）

若X -> Y的置信度和支持度和X'->Y'的相同（其中X'是X的子集；Y'是Y的子集），则它们是冗余的（只需要保留一条）。利用闭频繁项集能够避免产生这种冗余规则（能够避免两规则的前件为支持度相同的父子关系）

# 产生频繁项集的其他方法

随着事务数据宽度的增加，Apriori算法性能显著降低，下面介绍其他方法

## 项集格遍历

项集格类似于下图
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\9.png %}
基于此结构可以有不同的遍历方式

### 一般到特殊/特殊到一般

Apriori算法使用的是“一般到特殊的”的搜索策略，从上至下搜索（适用于频繁项集的最大长度不是太长的情况）
相反的，特殊到一般指代自下而上，容易发现极大频繁项集（一旦发现，则免去检查其子集）
此外还有结合两者的……

上述三种适合的情况如下（黑点表示非频繁项集）
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\10.png %}

### 等价类

将格划分为两个互不相交的结点组（等价类），依次在每个等价类内搜索，Apriori也可看成一种等价类（分层），下图是按前缀划分
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\11.png %}

### 宽度优先与深度优先

Apriori使用的是宽度优先；深度优先适合用于发现极大频繁项集（更快达到频繁项集边界），加以利用可以减小搜索空间

### 事务数据集的表示

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\12.png %}

# FP增长算法

不同于Apriori算法的“产生-测试”，FP增长算法基于FP树的紧凑**数据结构**组织数据，直接从结构中提取频繁项集。

## FP树表示法

将每条事务映射为FP树中的一条路径（不同路径重叠越多，压缩效果越好）
构造树之前对每个项（是项，不是项集）都计算一遍支持度，丢弃非频繁项，将频繁项按支持度降序排序，如下图排序后为abcde（项的排序方式影响树的大小，事实上不一定降序就一定好）
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\13.png %}
构造过程如上（虚线含义下节解释）
通常FP树大小比未压缩的数据小

## FP增长算法的频繁项集产生

# 关联模式的评估

很小的数据集就能产生巨大的关联规则，建立一组评估标准很重要
第一组为**客观**兴趣度度量（基于统计论据），如支持度、置信度和相关性；第二组为**主观**论据，即该规则（或模式）是不是有趣的，如黄油->面包就是无趣的，因为关系显而易见，将主观知识加入模式评价需要来自领域专家的大量先验知识

将主观信息加入模式发现任务的方法：

 1. 可视化：需要友好的环境，保持用户参与，允许专家解释和检验被发现的模式，与数据挖局系统交互
 2. 基于模板的方法：允许用户限制挖掘算法提取，意味着只把满足用户指定模板的规则提供给用户，而不是报告所有模式
 3. 主观兴趣度度量：基于领域信息定义

## 兴趣度的客观度量

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\16.png %}


### 支持度-置信度框架的局限性
首先是一些潜在有意义的低支持度的项被删去
其次，置信度存在缺陷，可以看下面的例子
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\14.png %}
就茶->咖啡这一关联规则，我们发现其支持度和置信度都相当高，那么是不是可以得出喜欢喝茶的也喜欢喝咖啡了呢？

但是，如果就比例上来看，所有人中喜欢喝咖啡的比例是80%，但在喜欢喝茶的人中这个比例降到了75%，与先前的推论矛盾。

置信度度量忽略规则后件的支持度，实际上喝咖啡的人本就很多。

### 其他几种客观度量
 
#### 兴趣因子及其局限性

$$提升度:lift(A\to B)=\frac{c(A\to B)}{s(B)}$$
$$兴趣因子I(A,B)=\frac{s(A,B)}{s(A)\times s(B)}=\frac{Nf_{11}}{f_{1+}f_{+1}}(针对二元变量)$$

当兴趣因子大于1时，A和B正相关（计算上面喝茶的例子，可以发现喝茶与喝咖啡是负相关的）

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\15.png %}

基于上图的例子，我们通过兴趣因子计算会发现pq反而接近相互独立，而rs关联强（而事实上pq一起出现的比例貌似更大）；这个时候换为置信度度量貌似更为合理。

#### 相关分析及其局限性

分析一对变量之间基于统计学的关系，关于连续变量的可以参考邻近性度量的皮尔森相关系数；对于二元变量，相关度度量方法如下（范围从-1到1
）$$\phi=\frac{f_{11}f_{00}-f_{01}f_{10}}{\sqrt{f_{1+}f_{+1}f_{0+}f_{+0}}}$$

同样利用上面的例子，发现pq和rs的$\phi$系数相等。局限性在于这种方法评估时项出不出现的重要性同样重要（适合于对称二元变量），此外，受样本大小影响

#### IS度量及其局限性

用于处理非对称二元变量$$IS(A,B)=\sqrt{I(A,B)\times s(A,B)}=\frac{s(A,B)}{\sqrt{s(A)s(B)}}$$

基于这种度量计算上个例子，这时能够得出pq的IS值是0.946，rs的IS值是0.286（符合我们的想法）

IS度量也可以表示为从一对二元变量中提取出的关联规则的置信度的几何均值（接近于较小的数）
$$IS(A,B)=\sqrt{\frac{s(A,B)}{s(A)} \times \frac{s(A,B)}{s(B)}}=\sqrt{c(A\to B)\times c(B\to A)}$$

只要规则中的一个方向置信度低就会导致低IS，证明IS的鲁棒性。

但对于一对互相独立的项集A和B的IS值是$\sqrt{s(A)\times s(B)}$，意味着不相关的模式下IS值也能很大（实际上负相关也可以）

### 度量的对称性与非对称性

对于度量M来说，若M(A->B)=M(B->A)则称其为对称的（常用于评价项集），反之非对称（适合于分析关联规则）
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\17.png %}

### 客观度量的性质

不同度量的结果可能是矛盾的，需要考察他们的性质从而了解差异

#### 反演性

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\18.png %}
> 反演性：如果交换频度计数f11和f00、f10和f01，客观度量M值不变，则其在反演操作下是不变的，

对于上图来说，就会出现向量C和D之间的$\phi$系数与A和B之间的相等，因为$\phi$系数在反演操作下不变，因而不适用于非对称的二元数据。

#### 零加性

> 零加性：增加f00而保持相依表所有其他的频度不变，不影响M的值，则称客观度量M在零加操作下不变

通俗来讲，往数据中加入无关数据不影响原度量的值。在文档分析或购物篮分析一般都喜欢满足零加不变，因为会有大量的不相关数据。

#### 缩放性

> 缩放不变性：对相依表的行/列做缩放而不影响客观度量M的值

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\19.png %}

## 辛普森悖论

> 辛普森悖论：隐藏变量可能会导致我们分析的变量之间的关联改变（联系消失或逆转方向）

{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\20.png %}

计算得出买了HDTV的人更有可能健身器。
但是当我们深入分析：
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\21.png %}

这个时候分层进行计算，会发现对于大学生来说不买HDTV更可能买健身器，对于在职人员也是如此，结果完全与上面的矛盾。

上述例子是典型的关联方向上的逆转，主要问题在于隐藏变量的影响，也提示我们需要适当分层去避免辛普森悖论。

# 倾斜支持度分布的影响

倾斜指代不平衡，这里的意思在于不同项支持度的不平衡，那么将会导致一系列问题：

首先是支持度的阈值难以设定，太高就会导致过滤掉低支持度而有价值的项（如商品中的珠宝）；太低则增加了关联分析的计算量，更关键的是会提取出一种虚假模式——交叉支持（cross-support）模式。

**交叉支持**，意为互相支持，表示寻找频繁项集时，高支持度的项给与低支持度项的支持，使得它们共同被挖掘，但事实上它们大部分是弱相关的。

> 通过一些兴趣度度量能区分这种现象，但是我们想要的是在挖掘频繁项集阶段就过滤掉，不给关联分析阶段增加计算量

针对交叉支持模式一个项集$X=\{i_1,i_2,i_3,...,i_k\}$，它的支持度比率r(X)小于阈值$h_c$$$r(X)=\frac{min[s(i_1),s(i_2),...,s(i_k)]}{max[s(i_1),s(i_2),...,s(i_k)]}$$

上面讲了，支持度无法过滤，那置信度呢？
{% qnimg 数据挖掘导论笔记-4-关联分析-基本概念和算法\22.png %}

高置信度并不能过滤掉上图中规则q->p

综上，在挖掘频繁项集阶段，置信度与支持度都无法过滤掉交叉支持模式

> 看到这里可能会有疑问，上面那个支持度比率不是就可以利用起来吗？先往下看

仔细观察就能知道p->q的支持度很低，那么我们对项集的过滤就可以加入下述操作以过滤交叉支持

计算频繁项集的最低置信度，称为h置信度或全置信度$$\frac{s(\{i_1,i_2,...,i_k\})}{max[s(i_1),s(i_2),...,s(i_k)]}$$

容易发现h置信度的上界是支持度比率，那我们就可以确保h置信度超过$h_c$去消除交叉支持，同时h置信度还具备反单调性
$$h\_confidence(\{i_1,i_2,...,i_k\}) \geq h\_confidence(\{i_1,i_2,...,i_k,i_{k+1}\})$$

> 我认为它的反单调性就是不用支持度比率的原因，因为h置信度的反单调性可以在挖掘项集过程也减少不少计算量
